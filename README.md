# Conversational-AI-Chatbot-Utilizing-BERT-Based-Emotion-Recognition
# Data Preprocessing Module

## 1. Project Overview
The Data Preprocessing Module is a critical component in our production-grade machine learning pipeline. It is designed to ingest raw datasets such as CSV files, clean and normalize data, tokenize text, and extract features to prepare data for modeling. The module is built to handle diverse data types including textual, numerical, and categorical inputs.

## 2. Directory Structure
```
.
├── goemotions_1.csv
├── goemotions_2.csv
├── goemotions_3.csv
├── Preprocessed_goemotions_1.csv
├── Preprocessed_goemotions_2.csv
├── Preprocessed_goemotions_3.csv
├── preprocess.ipynb
├── preprocess2.ipynb
└── preprocess3.ipynb
```
- **Raw Data Files:** Original CSV files with the raw data.
- **Preprocessed Data Files:** CSV files generated by the preprocessing steps.
- **Jupyter Notebooks:** Notebooks used to develop and validate preprocessing workflows.

## 3. Installation Instructions

- **Dependencies:**
  - Python 3.8 or higher
  - Libraries: Pandas, NumPy, scikit-learn, nltk (and additional dependencies as specified in `requirements.txt`)

- **Environment Setup:**
  - **Using Conda:**
    ```bash
    conda create -n preprocessing_env python=3.8
    conda activate preprocessing_env
    pip install -r requirements.txt
    ```
  - **Using Pip:**
    ```bash
    python -m venv preprocessing_env
    preprocessing_env\Scripts\activate
    pip install -r requirements.txt
    ```

- **Hardware Requirements:**
  - Minimum 4-core CPU
  - GPU is optional for accelerated processing

## 4. Usage Guide

Run the preprocessing module from the command line or via a Python script:

- **Command-Line Usage:**
  ```bash
  python preprocess.py --input goemotions_1.csv --output Preprocessed_goemotions_1.csv --config config.yaml
  ```

- **Python Module Example:**
  ```python
  from preprocessing import preprocess_data

  input_file = "goemotions_1.csv"
  output_file = "Preprocessed_goemotions_1.csv"
  config_file = "config.yaml"

  preprocess_data(input_file, output_file, config_file)
  ```

## 5. Preprocessing Steps

The module implements a sequence of systematic preprocessing steps:

- **Data Cleaning:**
  - Handling missing values and duplicates
  - Removing outliers

- **Normalization:**
  - Scaling numerical features
  - Standardizing categorical variables

- **Tokenization:**
  - Splitting text into tokens using libraries like [nltk](https://www.nltk.org/)
  - Removing stop words and punctuation

- **Feature Extraction:**
  - Deriving features based on the cleaned and normalized data
  - Utilizing scikit-learn transformers and custom functions
